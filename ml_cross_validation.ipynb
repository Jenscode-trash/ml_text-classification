{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "from data_preprocess import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and combine train_pos + train_neg\n",
    "PATH_POS = 'twitter-datasets/train_pos.txt'\n",
    "PATH_NEG = 'twitter-datasets/train_neg.txt'\n",
    "PATH_TEST = 'twitter-datasets/test_data.txt'\n",
    "PATH_COMBINE = 'twitter-datasets/train_combine.txt'\n",
    "\n",
    "data_pos = \"\"\n",
    "data_neg = \"\"\n",
    "\n",
    "with open(PATH_POS) as fp: \n",
    "    data_pos = fp.read() \n",
    "    \n",
    "with open(PATH_NEG) as fp: \n",
    "    data_neg = fp.read() \n",
    "\n",
    "data = data_pos + data_neg \n",
    "  \n",
    "with open (PATH_COMBINE, 'w') as fp: \n",
    "    fp.write(data)\n",
    "\n",
    "# data preprocess\n",
    "PATH_TRAIN_DATA = \"twitter-datasets/train_preprocess\"\n",
    "PATH_TEST_DATA = \"twitter-datasets/test_preprocess\"\n",
    "data_process(PATH_COMBINE, PATH_TRAIN_DATA, PATH_TEST, PATH_TEST_DATA)\n",
    "\n",
    "# read data\n",
    "train_data = read_train(PATH_TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FastText word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use continuous bag of words for word embedding\n",
    "dimension = 100\n",
    "model = fasttext.train_unsupervised(PATH_TRAIN_DATA, model = 'cbow', dim=dimension)\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])\n",
    "\n",
    "# create our final training data\n",
    "x_train = compute_word_embedding(model, train_data, dimension, vocabulary)\n",
    "y_train = [1] * 100000 + [0] * 100000 # change the number to 1250000 if you want to use full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64852 0.64968 0.64988 0.64354]\n",
      "0.647905\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "scores = cross_val_score(classifier, x_train, y_train, cv=4, scoring='accuracy')\n",
    "scores_mean = scores.mean()\n",
    "\n",
    "print(scores)\n",
    "print(scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7663  0.76078 0.75726 0.75944]\n",
      "0.7609450000000001\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# run grid search cross validation to find best parameters\n",
    "'''rf_param = {\n",
    "    'n_estimators': [100, 300, 500, 700, 1000],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=rf_param, scoring='accuracy', cv=4, n_jobs=1, verbose=10)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# view the best parameters and score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)'''\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=300)\n",
    "scores = cross_val_score(classifier, x_train, y_train, cv=4, scoring='accuracy')\n",
    "scores_mean = scores.mean()\n",
    "\n",
    "print(scores)\n",
    "print(scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7957  0.79472 0.79212 0.79032]\n",
      "0.793215\n"
     ]
    }
   ],
   "source": [
    "# SVM \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# run grid search cross validation to find best parameters\n",
    "'''param_svm = {'C': [0.001, 0.01, 0.1, 1], 'gamma' : ['scale', 'auto'], 'kernel':['linear', 'rbf']}\n",
    "classifier = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_svm, cv=4, n_jobs=1, verbose=10)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# view the best parameters and score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)'''\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "scores = cross_val_score(classifier, x_train, y_train, cv=4, scoring='accuracy')\n",
    "scores_mean = scores.mean()\n",
    "\n",
    "print(scores)\n",
    "print(scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
