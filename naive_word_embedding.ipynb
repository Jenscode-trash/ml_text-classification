{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import helpers\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_POS = '../twitter-datasets/train_pos.txt'\n",
    "PATH_NEG = '../twitter-datasets/train_neg.txt'\n",
    "PATH_OUT = '../twitter-datasets/train_combine.txt'\n",
    "\n",
    "data_pos = \"\"\n",
    "data_neg = \"\"\n",
    "\n",
    "with open(PATH_POS) as fp: \n",
    "    data_pos = fp.read() \n",
    "    \n",
    "with open(PATH_NEG) as fp: \n",
    "    data_neg = fp.read() \n",
    "    \n",
    "data = data_pos + data_neg \n",
    "  \n",
    "with open (PATH_OUT, 'w') as fp: \n",
    "    fp.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_DATA = '../twitter-datasets/train_combine.txt'\n",
    "dimension = 100\n",
    "\n",
    "model = fasttext.train_unsupervised(PATH_TRAIN_DATA, model = 'cbow', dim=dimension)\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEST_DATA = '../twitter-datasets/test_data.txt'\n",
    "\n",
    "ids, querys = helpers.read_test(PATH_TEST_DATA)\n",
    "train_data = helpers.read_train(PATH_TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_embedding(model, data, dimension):\n",
    "    ret_word_embedding = np.zeros((len(data), dimension))\n",
    "\n",
    "    for i, sentence in enumerate(data):\n",
    "        words = sentence.split(sep = ' ')\n",
    "        count = 0\n",
    "        avg_word_vector = np.zeros(dimension)\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                avg_word_vector = np.add(avg_word_vector,model[word])\n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            avg_word_vector = avg_word_vector / count\n",
    "        ret_word_embedding[i] = avg_word_vector\n",
    "    \n",
    "    return ret_word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = compute_word_embedding(model, querys, dimension)\n",
    "x_train = compute_word_embedding(model, train_data, dimension)\n",
    "y_train = [1] * 100000 + [0] * 100000 # change the number to 1250000 if you want to use full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter = 1000)\n",
    "lr.fit(x_train, y_train)\n",
    "predictions = lr.predict(x_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(predictions==0, -1, predictions)\n",
    "OUTPUT_PATH = '../twitter-datasets/submission.csv'\n",
    "\n",
    "helpers.create_csv_submission(ids, predictions, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
